{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from time import sleep\n",
    "from bs4 import BeautifulSoup\n",
    "from new_listing_scrape import basically_a_con\n",
    "from _pile import addresses, hdotp, new_pleasanton_short_link, nol\n",
    "\n",
    "\n",
    "'''\n",
    "returns every address from scrape of pages 1-21 of new pleasanton listings\n",
    "'''\n",
    "\n",
    "\n",
    "def pull_the_new_pleasanton_listings(base_url):\n",
    "    # determine how many pages deep this will go\n",
    "    r = basically_a_con(base_url)\n",
    "    # set variable\n",
    "    n_pages = determine_pages(r)\n",
    "    \n",
    "    \n",
    "    # master list\n",
    "    fly_out = []\n",
    "    # make list of links to each page in the sub-domain\n",
    "    pages = [base_url + '/' + str(_) for _ in range(1, n_pages)]\n",
    "    # pull data from each of them\n",
    "    for page in pages:\n",
    "        # print(i)\n",
    "        sleep(2)\n",
    "        # define info hub\n",
    "        response = basically_a_con(page)\n",
    "        # url accessibility check\n",
    "        if response is not None:\n",
    "            # define content\n",
    "            html = BeautifulSoup(response, hdotp)\n",
    "            # define out-route\n",
    "            _ = set()\n",
    "            for ul in html.select(addresses):\n",
    "                # 12 addresses (max seen)\n",
    "                for info in ul.text.split('\\n'):\n",
    "                    # is it legit\n",
    "                    if len(info) > 0:\n",
    "                        # print(_)\n",
    "                        # tag & bag\n",
    "                        _.add(info.strip())\n",
    "                    # else:\n",
    "                    #     continue\n",
    "            # update\n",
    "            fly_out.append(list(_))\n",
    "    # touchdown\n",
    "    return fly_out\n",
    "\n",
    "\n",
    "# ß = pull_the_new_pleasanton_listings(new_pleasanton_short_link)\n",
    "# ß"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_pages(response):\n",
    "    # url accessibility check\n",
    "    if response is not None:\n",
    "        # define content\n",
    "        html = BeautifulSoup(response, hdotp)\n",
    "        # define out-route\n",
    "        _ = set()\n",
    "        for ul in html.select(nol):\n",
    "            # 12 addresses (max seen)\n",
    "            for info in ul.text.split('\\n'):\n",
    "                # is it legit\n",
    "                if len(info) > 0:\n",
    "                    # tag & bag\n",
    "                    _.add(info.strip())\n",
    "        # storage\n",
    "        numbers = set()\n",
    "        # Viewing 1-12 of 251 listings (account for thousands , break on space)\n",
    "        for _ in str(_).replace(',','').split(' '):\n",
    "            # nice collection of numbers \n",
    "            for i in '0123456789':\n",
    "                # you got any numbers, fam?\n",
    "                if i in _:\n",
    "                    # '1-12' , '251'\n",
    "                    numbers.add(_)\n",
    "        # call storage\n",
    "        for i in list( numbers ):\n",
    "            # determine '1-12' from '251'\n",
    "            i = i.split('-')\n",
    "            if len(i) == 1:\n",
    "                # ['251']\n",
    "                # how many listings are in the selection\n",
    "                total = int(i.pop())\n",
    "            else:\n",
    "                # ['1','12']\n",
    "                # how many listings are being displayed per page\n",
    "                per_page = int(i.pop())\n",
    "        # determine number of pages in view\n",
    "        n_pages = int( total / per_page )\n",
    "        # if number of listings doesn't evenly divide \n",
    "        if isinstance((total/per_page),float):\n",
    "            # add an extra page to make sure we get em all\n",
    "            n_pages += 1\n",
    "    return n_pages\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "determine_pages(basically_a_con(new_pleasanton_short_link))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_active_listings_san_francisco_city = 'https://loiscox.bhhsdrysdale.com/homes/for-sale/status-active/city-;San%20Francisco/dsort-n'\n",
    "determine_pages(basically_a_con(new_active_listings_san_francisco_city))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
